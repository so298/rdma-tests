大規模言語モデル学習に割り当てられている環境でECMPによる負荷分散が働いていないのではないかと思われる現象が観測されている. 実験結果についてはこちらを参照

大規模言語モデル学習に割り当てられている16の, ノード占有型VMがある
それらのVMがつながっているリーフスイッチが rnwl14 - rnwl19

それらのスイッチから100秒間に渡り,
* traffic (bwm-ng -o csv で取得できるCSVのbytes_in, bytes_out)
* packet drop (ifconfig で取得できる TX/RX dropped) を取得した

* 前者は bwm-ng -t 1000 -c 100 -o csv で100秒に渡り取得, bytes_out, bytes_in に出た100個のデータの総和が計測区間における合計 bytes_out, bytes_in とみなしている
* 後者は ifconfig; sleep 1 を100回繰り返して取得, 最後のdropped の値と最初の値の差を, 計測区間における drop 数とみなしている

このようにしてリーフスイッチの各インタフェースの traffic/packet drop 数を得る
(一部の)インターフェースを経由してつながっているノード名は, net show interface alias で得た結果のうち,

```
UP     swp17s0         Trunk/L2      R07_gx001-r2
```

のように4カラム目が RX??_gx???-r? のようになっている行から得る.
そうでない場合はインタフェース名を, 対向ノードの名前とみなす

同じノード(gx???)に複数のインタフェースでつながっているケースがある.
そのような場合, 上記のaliasが片方がR07_gx001-r2, もう片方がR07_gx001-r4のようになっているが, 描画が混雑するので両者を同じノードとみなしている. 両者の traffic/drop数を加算して表示している

そのようにしてノード間の traffic/drop数を得た後, 計測期間中のtrafficが30GB以上のリンクだけを表示している.
正確には,

rnwl14 - rnwl19, およびそれらと隣接したノード, spineスイッチのうち, 30GB以上のトラフィックがあったリンク, およびそれらリンクの頂点をグラフに表示している

そのようにして表示されたノード, leafスイッチ, spineスイッチは, 

* 現在大規模言語モデルの学習に参加している12ノード
* leafスイッチ rnwl14, rnwl16, rnwl18
* spineスイッチ swp3, swp6, sp8

であった.

なお, それらのリンクのtrafficはどれも100GB以上であった
30GBを下回るリンクで次点としては22GB, 6GBというものがあった
おそらく1ノード単体で何らかの計算をしている

グラフの描画がきれいになされるように, upリンク(ノード -> leaf スイッチ-> spineスイッチ)のtraffic/drop数とdown リンク(spineスイッチ -> leafスイッチ -> ノード)のそれを別に表示している

(前者の場合もspineスイッチが上に配置されているがグラフ描画ツールの都合)

[1] 1つ目の問題は, spineスイッチに負荷分散されているように見えないこと

まずdownリンクのグラフ(down.pdf)を見ると, あるspineスイッチからのdown trafficはただ一つのleafスイッチに向かっている. 
例えば swp8からのdown trafficはすべてrnw14である.
言い換えるとleafスイッチrnwl14配下の5ノード(gx001, gx006, gx008, gx011, gx014)宛の, spineを経由する通信はことごとくswp8を経由するということである.

アプリケーションが発生させている通信パターンは, 機械学習の勾配を交換するための12ノード間のall reduceで, おそらく ReduceScatter + AllGather を用いている. すなわち, 12ノードで all-to-allの通信が起こる.

したがって, それら5ノードあてのspine経由の通信は5 x 7 = 35対存在している.
ECMPのハッシュ値による負荷分散で, 多数(16)のspineがあるなかでこれらすべての対が同じspineスイッチを経由することは極めて確率が低いと思われるのですがいかがでしょうか?

swp6, spw3についても同様である.

up trafficのグラフを見ても, あるleafスイッチからのup trafficが2つのspineスイッチにしか分散していないことがわかる. それは結局, 宛先ノードがつながるleafスイッチが同じならばすべて同じspineを通るようにルーティングされている(宛先ノードを使って負荷分散のハッシュが計算されているように見えない)という, down trafficで見られた挙動と一致している

[2] もう1つの問題は, 大量のpacket落ちが発生していることである

leafスイッチからspineスイッチへのupリンク, leafスイッチからノードへのdownリンクで, 100秒の間に5万〜50万のpacket落ちが発生している

(両者とも, leafスイッチにおけるifconfig TX droppedの値から得たもの)


なお, この現象を他のVMで再現しようと, CPU スポットVMを作って, 8 - 8の通信を同時に流したが, こちらの方では問題は再現しなかった. 引き続き実験を行って現象が起こる条件を精査する予定

- CPU VM (出ないことを確認済み)
- GPU VM 大規模言語モデル実験専用ノード (出ることを確認済み)

以降,

- GPU VM のホストメモリ間通信
- GPU VM のGPU Direct通信NCCL
- GPU VM のGPU Direct通信MPI+UCX

などのパターンを試す予定



